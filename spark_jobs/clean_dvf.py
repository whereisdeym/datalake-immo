from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# ðŸ”¹ Initialisation Spark
spark = SparkSession.builder.appName("CleanDVF").getOrCreate()

# ðŸ”¹ Lecture des donnÃ©es brutes DVF
RAW_PATH = "gs://datalake-immo-007/raw/dvf/dvf_2023.csv.gz"
print("Lecture des fichiers DVF...")
df = spark.read.option("header", "true").option("inferSchema", "true").csv(RAW_PATH)

# ðŸ”¹ VÃ©rif : affichons les colonnes lues
print("Colonnes disponibles :", df.columns)

# ðŸ”¹ Nettoyage : sÃ©lection des colonnes pertinentes
print("Nettoyage des donnÃ©es...")
df_clean = df.select(
    col("date_mutation"),
    col("valeur_fonciere"),
    col("code_commune"),
    col("surface_reelle_bati").alias("surface_m2"),
    col("type_local"),
    col("code_departement"),
    col("nom_commune")
)

# ðŸ”¹ Suppression des lignes vides et des doublons
df_clean = df_clean.dropna(how="any").dropDuplicates()

# ðŸ”¹ Ã‰criture dans le dossier "clean" du bucket
OUTPUT_PATH = "gs://datalake-immo-007/clean/dvf/"
print(f"Sauvegarde des donnÃ©es nettoyÃ©es vers {OUTPUT_PATH}")
df_clean.write.mode("overwrite").parquet(OUTPUT_PATH)

print("Nettoyage terminÃ© avec succÃ¨s !")

spark.stop()
