# ğŸ—ï¸ Immo Data Lake â€” Centralisation et Analyse des DonnÃ©es ImmobiliÃ¨res FranÃ§aises (DVF)

## ğŸ“˜ Contexte du projet
Ce projet a Ã©tÃ© conÃ§u dans le cadre de mon apprentissage en **Data Engineering** et vise Ã  construire un **Data Lakehouse complet sur Google Cloud Platform (GCP)**.

Lâ€™objectif : **centraliser, nettoyer et historiser les transactions immobiliÃ¨res franÃ§aises (donnÃ©es DVF)** pour permettre des analyses temporelles (Ã©volution des prix, surfaces, types de biens, etc.).

---

## ğŸ§± Architecture du projet

**Technologies principales :**
- **Google Cloud Storage (GCS)** â€“ stockage des donnÃ©es brutes et traitÃ©es  
- **Dataproc (PySpark)** â€“ traitement, nettoyage et transformation  
- **Delta Lake** â€“ stockage optimisÃ©, versionnÃ©, avec Time Travel  
- **Dataplex** â€“ documentation, catalogage et gouvernance des donnÃ©es  

**Structure du Data Lake :**
immo-data-lake/
â”œâ”€â”€ raw/ â†’ donnÃ©es brutes (CSV DVF)
â”œâ”€â”€ cleaned/ â†’ donnÃ©es nettoyÃ©es (Parquet)
â””â”€â”€ delta/ â†’ tables Delta Lake partitionnÃ©es (2020â€“2024)


**Flux de transformation :**

**Vue dâ€™ensemble Dataplex :**

<p align="center">
  <img src="architecture/immo_data_lake_diagram.png" width="700" alt="Architecture du Data Lake DVF sur GCP"/>
</p>

---

## âš™ï¸ Pipeline de traitement

### 1ï¸âƒ£ Ingestion (Raw Zone)
TÃ©lÃ©chargement automatique des fichiers DVF (2020â€“2025) depuis [data.gouv.fr](https://www.data.gouv.fr/fr/datasets/demandes-de-valeurs-foncieres/), stockage dans **GCS (raw/)**.

> Script : [`scripts/dvf_ingest.py`](scripts/dvf_ingest.py)

### 2ï¸âƒ£ Nettoyage (Cleaned Zone)
Traitement PySpark sur Dataproc :
- Lecture des fichiers bruts CSV
- SÃ©lection et normalisation des colonnes
- Conversion en Parquet

> Script : [`scripts/clean_dvf.py`](scripts/clean_dvf.py)

### 3ï¸âƒ£ Stockage Delta Lake (Delta Zone)
CrÃ©ation dâ€™une table Delta partitionnÃ©e par `annee` et `code_commune`, avec gestion des versions.

> Script : [`scripts/create_delta_table.py`](scripts/create_delta_table.py)

---

## ğŸ§­ Gouvernance & Documentation (Dataplex)

Les donnÃ©es sont intÃ©grÃ©es dans **Dataplex** :
- **Lake** : `immo-lake`
- **Zones** : `raw-zone`, `cleaned-zone`, `delta-zone`
- **Assets GCS** : reliÃ©s automatiquement avec dÃ©tection du schÃ©ma
- **Tags** : source, owner, description, date de mise Ã  jour
- **DÃ©couverte automatique** : scan toutes les 12h

ğŸ“˜ Documentation Dataplex : [`dataplex/lake_config.md`](dataplex/lake_config.md)

---

## ğŸ“Š Exemple dâ€™analyse (Notebook)

Un notebook Jupyter permet dâ€™explorer les donnÃ©es Delta :
- Moyenne du prix au mÂ² par commune et annÃ©e
- Comparaison dâ€™Ã©volution sur 6 mois
- AgrÃ©gations temporelles via SparkSQL

> Notebook : [`notebooks/dvf_analysis_example.ipynb`](notebooks/dvf_analysis_example.ipynb)

---

## ğŸ§¾ Exemple de rÃ©sultats

| AnnÃ©e | Commune                | Prix moyen au mÂ² (â‚¬) | Nb ventes |
|--------|------------------------|-----------------------|------------|
| 2020 | Lyon                   | 5150                 | 1125 |
| 2021 | Nantes                 | 3970                 | 948 |
| 2022 | Bordeaux               | 4680                 | 1021 |
| 2023 | Marseille              | 3650                 | 1342 |

*(RÃ©sultats calculÃ©s sur un Ã©chantillon simplifiÃ©)*

---

## ğŸ§° Stack technique

| Composant | Technologie |
|------------|-------------|
| Cloud | Google Cloud Platform (GCP) |
| Stockage | Google Cloud Storage |
| Traitement | Apache Spark (Dataproc) |
| Format | Parquet + Delta Lake |
| Gouvernance | Dataplex |
| Langage | Python (PySpark) |
| Visualisation | Jupyter / SQL |

---

## ğŸ§‘â€ğŸ’» Auteur
ğŸ‘¤ **Deymar** â€” Ã‰tudiant en Master 1 Data Engineering Ã  Webtech INSTITUTE  
ğŸ¯ PassionnÃ© par le Big Data, les architectures Cloud et la DataOps  
ğŸ“« [LinkedIn](#) | [GitHub](#)

---

## ğŸ“š Ã€ propos
Ce projet sâ€™inscrit dans une dÃ©marche dâ€™apprentissage visant Ã  :
- comprendre les principes du Data Lakehouse moderne,
- maÃ®triser les pipelines PySpark sur Dataproc,
- apprendre la gouvernance des donnÃ©es avec Dataplex.

